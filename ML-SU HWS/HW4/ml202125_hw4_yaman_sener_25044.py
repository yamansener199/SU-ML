# -*- coding: utf-8 -*-
"""ML202125_HW4_YAMAN_SENER_25044.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qffotL1Q0sJtFhBEMnXXXSmM1Y3YNIUk

# **HOMEWORK 4**
---

**Goal:** Build a multilayer perceptron (MLP) model & for classifying SVHN dataset.

**Dataset:** SVHN, 32x32 RGB number images.        
http://ufldl.stanford.edu/housenumbers/

---

###**Instructions**             
**1)** **Preprocessing**  
> **1.1)** Load the dataset                 
> **1.2)** Normalize features                
  **1.3)** Visualize dataset

**2)** **Modelling**
> **2.1)** Try different fully hyperparameters (# of hidden layers, learning rate, # of epochs, # of neurons, add dropout, etc.)             
> **2.2)** Train with early stopping

**3)** **Report**                   
Share your results, which hyperparameters you used, train & test accuracy, etc. 
Write an at most 1/2-page summary of your approach to this problem at the end of your notebook; this should be like an executive summary. Include problem definition and preprocessing as well.


> **Topics to Discuss:**                                      
Explain you results.           
How did you choose the best hyperparameters?                         
What happened when the # of epochs are too large/small, why?                  
What happened when the learning rate is too large/small, why?              
What did you observe when you change the # of hidden layers?                
What did you observe when you change the # of neurons?   
What is the use of adding dropout?                     
What is the use of early stopping? 


---


**Note:** Don't forget to change Colab's runtime to GPU.

#1) Preprocessing
Load, normalize and visualize data


---
"""

from keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from keras.layers import Dense, Flatten
from keras.optimizers import Adam, SGD
from matplotlib import pyplot as plt
from keras.models import Sequential
from keras.layers import Dropout
from google.colab import drive
from tensorflow import keras
from matplotlib import pyplot
!pip install -q -U keras-tuner # Inside Google colab Linux commands are ran by prefixing '!'
import kerastuner as kt
import scipy.io as sio
import numpy as np
import numpy
drive.mount('/content/drive')

# Load SVHN dataset

def load_data(path):
    data = sio.loadmat(path)
    return data['X'], data['y']

X_train, y_train = load_data('/content/drive/MyDrive/train_32x32.mat')
X_test, y_test = load_data('/content/drive/MyDrive/test_32x32.mat')

# Summarize dataset (count, shape, min/max value)
label, count = np.unique(y_train, return_counts=True)
print("Occurances of labels in training set:",dict(zip(label, count)))
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)
print('Train X Min: {}, Max: {}'.format(X_train.min(), X_train.max()))
print('Train Y Min: {}, Max: {}'.format(y_train.min(), y_train.max()))

#Change input shape

X_train = np.moveaxis(X_train, -1, 0)
X_test = np.moveaxis(X_test, -1, 0)
y_train=y_train.reshape((y_train.shape[0]))
y_test= y_test.reshape((y_test.shape[0]))
print("Shape of X train after fixation: ", X_train.shape)
print("Shape of X test after fixation: ", X_test.shape)
print("Shape of Y train: ", y_train.shape)
print("Shape of X train: ", X_train.shape)

# Normalize dataset
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255

print('Min: {}, Max: {}'.format(X_train.min(), X_train.max()))

# Visualize some samples
for i in range(9):
	pyplot.subplot(331 + i)
	pyplot.imshow(X_train[i], cmap=pyplot.get_cmap('gray'))
pyplot.show()

X_train = X_train.reshape(X_train.shape[0], -1)
X_test = X_test.reshape(X_test.shape[0],-1)
y_train[y_train > 9] = 0
y_test[y_test > 9] = 0

"""#2) Modelling
(Build, compile, fit, evaluate)


---

##2.1) Part 1

---
"""

# Build the model
model = Sequential()
model.add(Dense(512,
                activation='relu',
                name='hidden_layer1',
                input_shape=(X_train.shape[1],)))
model.add(Dense(128,
                activation='relu',
                name='hidden_layer2'))
model.add(Dense(10,
                activation='softmax',
                name='output_layer'))

model.summary()

# Compile & fit the model 
model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])
history = model.fit(X_train, y_train, batch_size=256, epochs=10, validation_split = 0.2, verbose=1)
# Hint: You can assign batch sizes 128, 256

# Evaluate the model on test data
score = model.evaluate(X_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

# Try different hyperparameters and observe the results
def model_builder(hp):
  
  model = keras.Sequential()
  hp_units = hp.Choice('n_neurons', values=[64, 128, 512])
  
  model.add(Dense(units=hp_units,
                  name='input_layer',
                  input_shape=(X_train.shape[1],)))
  
  for i in range(hp.Int("n_hidden_layers",2,3)):
    model.add(Dense(units=hp_units, activation='relu', name=f"hidden_layer{i}"))

  model.add(Dense(10,
                  activation='softmax',
                  name='output_layer'))
  hp_learning_rate = hp.Choice('learning_rate',values=[1e-1, 1e-6])

  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),
                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=['accuracy'])
  
  return model


tuner = kt.RandomSearch(
                  model_builder,
                  objective='val_accuracy',
                  max_trials = 5,
                  executions_per_trial = 1,
                  overwrite=True
                  )
  


tuner.search(X_train, y_train, epochs=10, validation_split=0.2)
print(tuner.get_best_hyperparameters()[0].values)
print(tuner.get_best_models()[0].summary())

"""


---


**Dropout** : [Dropout](https://keras.io/api/layers/regularization_layers/dropout/) is a regularization approach. At each training stage, specified percent of individual nodes are dropped out of the network.

[Read](https://machinelearningmastery.com/how-to-reduce-overfitting-with-dropout-regularization-in-keras/) MLP Dropout Regularization for further information

![picture](https://drive.google.com/uc?id=12lv5VsPFLt6sKEZvgQRyQ7mHeW4l-Vq9)"""

#Try adding dropout to the architecture
model = Sequential()
model.add(Dense(512,
                activation='relu',
                name='hidden_layer1',
                input_shape=(X_train.shape[1],)))
model.add(Dropout(0.2))
model.add(Dense(128, 
                activation='relu',
                name='hidden_layer2'))
model.add(Dense(10, 
                activation='softmax',
                name='output_layer'))

# Compile & fit the model 
model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])

# Hint: You can assign batch sizes 128, 256
history = model.fit(X_train,
                    y_train,
                    batch_size=256,
                    epochs=10,
                    validation_split = 0.2,
                    verbose=1)
score = model.evaluate(X_test, y_test, verbose=1)
print('Test acc :', score[1]) 
print('Test los :', score[0])

"""##2.2) Part 2


---

When training a model, model stops generalizing after a while and learns noices of the datapoints as well. One way to avoid this **overfitting** is to use **early stopping**. With early stopping, model performance is monitored on validation data in each epoch and stops updating weights when validation performance starts decreasing.

https://keras.io/api/callbacks/early_stopping/

![picture](https://drive.google.com/uc?id=1Rs8FkpVgifspzvlIfdTDYyBVjR01OgVj)
"""

# Build & compile the model
new_model = tuner.get_best_models()[0]
new_model.summary()

best_model = tuner.get_best_models()[0]
print(best_model.summary())

"""![picture](https://drive.google.com/uc?id=1Mp2W2VVQVmyuleSDNDKJLI6hxAtyG5gx)"""

#Plot train & validation loss
#Can't compile because of performance issues
es = EarlyStopping(monitor='val_loss', patience=2, verbose=1, mode='min',restore_best_weights=True) # Training will stop when minimum validation loss is achieved
new_model.fit(X_train, y_train, batch_size=128, epochs=100, validation_split = 0.2, callbacks=[es], verbose=1)
plt.plot(new_model.history.history['loss'])
plt.plot(new_model.history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

#Evaluate on test data
result = new_model.evaluate(X_test, y_test, verbose=1)
print('Test loss : ', result[0])
print('Test acc : ', result[1])

"""#3) Report


---

Summary of models you try (learning rate, # of hidden layers, # of epochs, test accuracy, etc.) Write a short report & discuss the results of the models you trained

We have used a tuner by performing random search on following hyperparameter sets:n_neurons ,n_hidden_layers,learning_rate and n_epochs.So whole process starts with the tuners selection.The selection is based on random configurations from hyperparameter sets we already decided.In addition to that , tuner also chosesess the best hyperparameters depending on the of validation accuracy.The model was underfitting because the number of epochs was too limited. To avoid overfitting, we expanded the number of epochs and used es(Early Stopping). When instruction is stopped early, the lowest validation failure is reached. The model did not learn correctly because the learning rate was too high. Choosing a slower but better learning rate resulted in a slower but better learning rate.The model became overfitting right after we expanded the number of hidden neurons,layers. To prevent overfitting, we used a different regularization approach.So basically DropOut behaves like ; randomly chosen neurons are overlooked during preparation with dropout.
"""