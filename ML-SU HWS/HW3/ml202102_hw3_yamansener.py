# -*- coding: utf-8 -*-
"""ML202102_HW3_YAMANSENER.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L4-EeppU3MOZh9t-ujh-vfH81hRm1td8

# CS 412 Machine Learning 2020 

# Assignment 3

100 pts

## Goal 

The goal of this assignment 

*  Introduction to working with text data
*  Gain experience with the Scikit-Learn library
*  Gain experience with Naive Bayes and Logistic Regression

## Dataset

**20 Newsgroup Dataset** is a collection 18846 documents which are about 20 different topics.


## Task
Build naive bayes and logistic regression classifiers with the scikit-learn library function to **classify** the documents about their content topic.

## Submission

Follow the instructions at the end.

# 1) Initialize

First, make a copy of this notebook in your drive

# 2) Load Dataset

The 20 Newsgroup Dataset exist on Scikit-Learn library.
"""

from sklearn.datasets import fetch_20newsgroups

train_batch = fetch_20newsgroups(subset='train')
test_batch = fetch_20newsgroups(subset='test')

# target groups you will be dealing with
target_groups = train_batch.target_names
target_groups

# creating training and test sets
train_x =  train_batch["data"]
train_y =  train_batch["target"]
test_x  =  test_batch["data"]
test_y  =  test_batch["target"]

print(train_x[0])

print(target_groups[train_y[0]])

"""# Preprocess"""

import re

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import nltk
# nltk.download("stopwords")

from nltk.corpus import stopwords
stop_words = stopwords.words("english")

from nltk.stem.snowball import SnowballStemmer
stemmer = SnowballStemmer("english")

# You will use this function to preprocess your data. If you would like to add another preprocessing step in the function, please add it and mention about it in your report.
def preprocess(text):
  text = re.sub("[\w\d._]+@[^\s]+|[^\s]+\.[^\s]+|[^\s]+-[^\s]+|\d+|[^\w\s]","",text.lower().strip())
  text = ' '.join([stemmer.stem(word) for word in re.findall("\w+",text) if word not in stop_words])
  return text

# Apply <preprocess> function on the training and test set 
i=0
while(i<len(train_x)):
  train_x[i]=preprocess(train_x[i])
  i=i+1

i=0
while(i<len(test_x)):
  test_x[i]=preprocess(test_x[i])
  i=i+1

"""# Models"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression

from sklearn.feature_extraction.text import CountVectorizer

from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV

from sklearn.metrics import accuracy_score

import numpy as np
import pandas as pd

"""## Tune Naive Bayes"""

# Create a CountVectorizer for NB with:
#     min_df = 50
#     max_df = 3000
#     stop_words = stop_words
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(min_df=50,max_df=3000,stop_words=stop_words)

# Vectorize your training and test set
train_x_naiveb = vectorizer.fit_transform(train_x)
test_x_naiveb = vectorizer.transform(test_x)

train_y_naiveb=train_y
test_y_naiveb=test_y

#https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html

#Initiate the NB model with required components.
nb_model = MultinomialNB()


#Set the hyperparameter space that will be scanned:
#   alpha = (0.1,0.5,1.0,5.0)
hyperparameters = dict(alpha = (0.1,0.5,1.0,5.0),)


#Let the GridSearchCV scan the hyperparameter and find the best hyperparameter set that will maximize the scoring option.
#   cv = 3
#   scoring = "accuracy"
mnb_gsearch = GridSearchCV(nb_model,hyperparameters,cv=3,scoring="accuracy",n_jobs=-1)
mnb_gsearch.fit(train_x_naiveb,train_y_naiveb)

# show the best score
mnb_gsearch.best_score_

# show the best parameter
mnb_gsearch.best_params_

"""### Evaluate The Best Model for NB"""

#Create your NB model with the best parameter set.

#Fit your model on training set.
best_model =MultinomialNB().set_params(**mnb_gsearch.best_params_)
best_model = best_model.fit(train_x_naiveb,train_y_naiveb)

# Make predictions on test set
predictions = best_model.predict(test_x_naiveb)

# Show your accuracy on test set
accuracy_score(test_y,predictions)

"""## Tune Logistic Regresion"""

# Create a CountVectorizer for LR with:
#     min_df = 50
#     max_df = 3000
#     stop_words = stop_words
vectorizer_lr = CountVectorizer(min_df=50,max_df=3000,stop_words=stop_words)

# Vectorizer your training and test set
trainx_lr = vectorizer_lr.fit_transform(train_x)
testx_lr = vectorizer_lr.transform(test_x)
trainy_lr=train_y
testy_lr=test_y

#https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html

#Initiate the LR model:
#   max_iter=2000
model_2 = LogisticRegression(max_iter=2000)

# Set the hyperparameter space that will be scanned:
#     C = (0.001,0.01,0.1,1)     
hyperparameters2 = dict(C = (0.001,0.01,0.1,1),)

#Let the GridSearchCV scan the hyperparameter and find the best hyperparameter set that will maximize the scoring option.
#   cv = 3
#   scoring = "accuracy"
mnb_gsearch_2 = GridSearchCV(model_2,hyperparameters2,cv=3,scoring="accuracy",n_jobs=-1)
model_2=mnb_gsearch_2.fit(trainx_lr,trainy_lr)

# show the best score
mnb_gsearch_2.best_score_

# show the best parameter
mnb_gsearch_2.best_params_

"""### Evaluate The Best Model for Logistic Regression"""

#Create your LR model with the best parameter set.

#Fit your model on training set.
model_2 =LogisticRegression(max_iter=2000).set_params(**mnb_gsearch_2.best_params_)
model_2 = model_2.fit(trainx_lr,trainy_lr)

# Make predictions on test set
predictions_2 = model_2.predict(testx_lr)

# Show your accuracy on test set
accuracy_score(test_y,predictions_2)

"""# Feature Importances"""

# Find the each category's most important top 3 features (words) for LR model and show with a dataframe
words = vectorizer_lr.get_feature_names()
cofs = model_2.coef_

list_empty=[]
number=3
for i in cofs :
  cofwords=dict(sorted(list(zip(i,words)),reverse=True ))
  step1=list(cofwords.items())[0:number]
  new_arr=np.asarray(step1)
  new_arr=np.delete(new_arr,[0,0],axis=1)
  liste=new_arr.tolist()
  list_empty.append(liste)
###DONE###

dump=dict(zip(target_groups,list_empty))
lr_df=pd.DataFrame.from_dict(dump)
lr_df

# Find the each category's most important top 3 features (words) for NB model and show with a dataframe
words_2 = vectorizer.get_feature_names()
cofs_2 = best_model.coef_

# It should look like this:
list_empty_2=[]
number=3
x=0
while(x < len(cofs)): 
  cofwords_2=dict(sorted(list(zip(i,words)),reverse=True ))
  step1=list(cofwords_2.items())[0:number]
  new_arr_2=np.asarray(step1)
  new_arr_2=np.delete(new_arr_2,[0,0],axis=1)
  liste_2=new_arr_2.tolist()
  list_empty_2.append(liste_2)
  x=x+1
###DONE###

dump_2=dict(zip(target_groups,list_empty_2))
nb_df=pd.DataFrame.from_dict(dump_2)
nb_df

"""# **Notebook & Report**

Notebook: We may just look at your notebook results; so make sure each cell is run and outputs are there.

Report: Write an at most 1/2 page summary of your approach to this problem at the end of your notebook; this should be like an abstract of a paper or the executive summary.

Must include statements such as:

( Include the problem definition: 1-2 lines )

(Talk about any preprocessing you did, explain your reasoning)

(Talk about train/test sets, size and how split)

(State what your test results are with the chosen method, parameters: e.g. "We have obtained the best results with the ….. classifier (parameters=....) , giving classification accuracy of …% on test data….")

(Comment on feature importances of models)

(Comment on anything that you deem important/interesting)


You will get full points from here as long as you have a good (enough) summary of your work, regardless of your best performance or what you have decided to talk about in the last few lines.

# **Write your report in this cell**
We already have specific catagories listed as target-groups and we will try to find the most common used for each of these catagories.In addition to that we will be using 2 different models.Everything started up with preprocessings and then we get the root of the words.With this method we will be holding all the words having the same root.We also removed unnecessary coefficients from the lists.Moreober we used 2 different models ;naive bayes and logisric regression.Then we tarined our model with constant hyperparameters.Both of the results coming from two models are slightly familiar with small differences.

..

..

..

# **Submission**
You will submit this homework via SUCourse.


Please read this document again before submitting it.

Please submit your **"share link" INLINE in Sucourse submissions.** That is we should be able to click on the link and go there and run (and possibly also modify) your code.

For us to be able to modify, in case of errors etc, you should get your "share link" as **share with anyone in edit mode** 

Download the **.ipynb and the .html** file and upload both of them to Sucourse.
 
Please do your assignment individually, do not copy from a friend or the Internet. Plagiarized assignments will receive -100.
"""