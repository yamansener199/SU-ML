# -*- coding: utf-8 -*-
"""cs_412_yamansener_hw2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PheGu0F1ebbFzsJMfhZxQw0oI9qvbyUu
"""

import numpy as np
import pandas as pd
from google.colab import drive
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from math import sqrt
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

drive.mount('/content/drive')

path_prefix = "/content/drive/My Drive"
housedata = pd.read_csv(join(path_prefix, "Real estate.csv"))
housedata.drop('No', axis=1, inplace=True)
housedata.tail(10)

X = housedata[['X1 transaction date','X2 house age','X3 distance to the nearest MRT station','X4 number of convenience stores','X5 latitude','X6 longitude']]
yy = housedata['Y house price of unit area']
y= pd.DataFrame(yy)
X

y.head()

linear_reg = LinearRegression()
  linear_reg.fit(X, y)
  X, X_test, y, y_test = train_test_split(X, y, test_size=0.15, random_state=42)
  print( "Full train label shape :", y.shape,"Full train data shape : ", X.shape,"Test label shape :  ", y_test.shape, "Test data shape : ", X_test.shape)
  X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.15, random_state=42)

linear_reg = LinearRegression()
linear_reg.fit(X_train, y_train)

"""#P1"""

ypred_train = linear_reg.predict(X_train)
ypred_valid = linear_reg.predict(X_valid)
lrmse = mean_squared_error(y_valid, ypred_valid)
print("Train Mean squared error : %.2f " % mean_squared_error(y_train, ypred_train))
print("Valid Mean squared error : %.2f " % lrmse)

x_axis = np.arange(ypred_valid.shape[0])
plt.scatter(x_axis,ypred_valid)
fig=plt.plot(x_axis,ypred_valid, 'r')

x_axis = np.arange(ypred_train.shape[0])
plt.scatter(x_axis,ypred_train)
fig=plt.plot(x_axis,ypred_train, 'r')

"""#P2"""

poly_regress = PolynomialFeatures(degree=3)
X_poly_train = poly_regress.fit_transform(X_train)
linear_reg_3 = LinearRegression()
linear_reg_3.fit(X_poly_train, y_train)
y_pred_train = linear_reg_3.predict(X_poly_train)
X_poly_valid = poly_regress.transform(X_valid)
X_poly_valid.shape
y_pred_valid = linear_reg_3.predict(X_poly_valid)
lrmse2 = mean_squared_error(y_valid, y_pred_valid)
print("Valid Mean squared error :",lrmse2)
print("Train Mean squared error :",mean_squared_error(y_train, y_pred_train))

x_axis = np.arange(y_pred_valid.shape[0])
plt.scatter(x_axis,y_pred_valid)
fig=plt.plot(x_axis,y_pred_valid, 'r')

x_axis = np.arange(y_pred_train.shape[0])
plt.scatter(x_axis,y_pred_train)
fig=plt.plot(x_axis,y_pred_train, 'r')

"""#P3"""

poly_regress = PolynomialFeatures(degree=3)
X_poly_train = poly_regress.fit_transform(X_train)
linear_reg_3 = LinearRegression()
linear_reg_3.fit(X_poly_train, y_train)
y_pred_train = linear_reg_3.predict(X_poly_train)
X_poly_valid = poly_regress.transform(X_valid)
X_poly_valid.shape
y_pred_valid = linear_reg_3.predict(X_poly_valid)
lrmse3 = mean_squared_error(y_valid, y_pred_valid)
print("Valid Mean squared error :",lrmse3)
print("Train Mean squared error :",mean_squared_error(y_train, y_pred_train))

x_axis = np.arange(y_pred_valid.shape[0])
plt.scatter(x_axis,y_pred_valid)
fig=plt.plot(x_axis,y_pred_valid, 'r')

x_axis = np.arange(y_pred_train.shape[0])
plt.scatter(x_axis,y_pred_train)
fig=plt.plot(x_axis,y_pred_train, 'r')

"""#P4"""

poly_regress = PolynomialFeatures(degree=4)
X_poly_train = poly_regress.fit_transform(X_train)
linear_reg_4 = LinearRegression()
linear_reg_4.fit(X_poly_train, y_train)
y_pred_train = linear_reg_4.predict(X_poly_train)
X_poly_valid = poly_regress.transform(X_valid)
X_poly_valid.shape
y_pred_valid = linear_reg_4.predict(X_poly_valid)
lrmse4 = mean_squared_error(y_valid, y_pred_valid)
print("Valid Mean squared error : ",lrmse4)
print("Train Mean squared error :",mean_squared_error(y_train, y_pred_train))

x_axis = np.arange(y_pred_valid.shape[0])
plt.scatter(x_axis,y_pred_valid)
fig=plt.plot(x_axis,y_pred_valid, 'r')

x_axis = np.arange(y_pred_train.shape[0])
plt.scatter(x_axis,y_pred_train)
fig=plt.plot(x_axis,y_pred_train, 'r')

"""#P5"""

poly_regress = PolynomialFeatures(degree=5)
X_poly_train = poly_regress.fit_transform(X_train)
linear_reg_5 = LinearRegression()
linear_reg_5.fit(X_poly_train, y_train)
y_pred_train = linear_reg_5.predict(X_poly_train)
X_poly_valid = poly_regress.transform(X_valid)
y_pred_valid = linear_reg_5.predict(X_poly_valid)
lrmse5 = mean_squared_error(y_valid, y_pred_valid)
print("Valid Mean squared error: %.2f " % lrmse5)
print("Train Mean squared error : %.2f "% mean_squared_error(y_train, y_pred_train))

x_axis = np.arange(y_pred_valid.shape[0])
plt.scatter(x_axis,y_pred_valid)
fig=plt.plot(x_axis,y_pred_valid, 'r')

x_axis = np.arange(y_pred_train.shape[0])
plt.scatter(x_axis,y_pred_train)
fig=plt.plot(x_axis,y_pred_train, 'r')

print('Linear regression mse:\t\t', lrmse,'\n2 degree polynomial mse:\t', lrmse2,'\n3 degree polynomial mse:\t', lrmse3,'\n4 degree polynomial mse:\t', lrmse4,'\n5 degree polynomial mse:\t', lrmse5,
      )

x_axis=[1,2,3,4,5]
y_axis=[lrmse,lrmse2,lrmse3,lrmse4,lrmse5]
plt.plot(x_axis,y_axis)
plt.show

"""Let's drop ,less related distances compared to MRT such as X3 """

X_train2=X_train.drop(['X5 latitude'],axis=1)
 X_valid2=X_valid.drop(['X5 latitude'],axis=1)

"""0 Table Dropping"""

poly_regress = PolynomialFeatures(degree=3)
X_poly_train = poly_regress.fit_transform(X_train)
linear_reg_3 = LinearRegression()
linear_reg_3.fit(X_poly_train, y_train)
y_pred_train = linear_reg_3.predict(X_poly_train)
X_poly_valid = poly_regress.transform(X_valid)
X_poly_valid.shape
y_pred_valid = linear_reg_3.predict(X_poly_valid)
lrmse_f0 = mean_squared_error(y_valid, y_pred_valid)
print("Valid Mean squared error:",lrmse_f0)
print("Train Mean squared error:",mean_squared_error(y_train, y_pred_train))

"""1 Table Dropping"""

poly_regress = PolynomialFeatures(degree=3)
X_poly_train = poly_regress.fit_transform(X_train2)
linear_reg_3 = LinearRegression()
linear_reg_3.fit(X_poly_train, y_train)
y_pred_train = linear_reg_3.predict(X_poly_train)
X_poly_valid = poly_regress.transform(X_valid2)
X_poly_valid.shape
y_pred_valid = linear_reg_3.predict(X_poly_valid)
lrmse_f1 = mean_squared_error(y_valid, y_pred_valid)
print("Valid Mean squared error : ",lrmse_f1)
print("Train Mean squared error :",mean_squared_error(y_train, y_pred_train))

"""2 Table Dropping"""

X_train2=X_train.drop(['X1 transaction date'],axis=1)
X_valid2=X_valid.drop(['X1 transaction date'],axis=1)

poly_regress = PolynomialFeatures(degree=3)
X_poly_train = poly_regress.fit_transform(X_train2)
linear_reg_3 = LinearRegression()
linear_reg_3.fit(X_poly_train, y_train)
y_pred_train = linear_reg_3.predict(X_poly_train)
X_poly_valid = poly_regress.transform(X_valid2)
X_poly_valid.shape
y_pred_valid = linear_reg_3.predict(X_poly_valid)
lrmse_f2 = mean_squared_error(y_valid, y_pred_valid)
print("Valid Mean squared error:",lrmse_f2)
print("Train Mean squared error:",mean_squared_error(y_train, y_pred_train))

X_train2=X_train.drop(['X1 transaction date'],axis=1)
X_valid2=X_valid.drop(['X1 transaction date'],axis=1)

"""3 Table Dropping"""

poly_regress = PolynomialFeatures(degree=3)
X_poly_train = poly_regress.fit_transform(X_train2)
linear_reg_3 = LinearRegression()
linear_reg_3.fit(X_poly_train, y_train)
y_pred_train = linear_reg_3.predict(X_poly_train)
X_poly_valid = poly_regress.transform(X_valid2)
X_poly_valid.shape
y_pred_valid = linear_reg_3.predict(X_poly_valid)
lrmse_f3 = mean_squared_error(y_valid, y_pred_valid)
print("Train Mean squared error:",mean_squared_error(y_train, y_pred_train))
print("Valid Mean squared error:",lrmse_f3)

"""The discarded columns were: X1 transaction date, X6 longitude, and X5 latitude, and our validation error decreased optimally with d=3."""

x_axis=[1,2,3,4]
y_axis=[lrmse_f0,lrmse_f1,lrmse_f2,lrmse_f3]
plt.plot(x_axis,y_axis)
plt.show

"""Our goal in this homework is to use the most evolved model to forecast the house price with the least amount of error, as well as to refine our hyperparameters to provide our model with the most appropriate parameters. We started by pruning the results and also removing the column "NO".Because it doesn't reflect anything useful for our goal.We're going to use linear regression and then we will make a linear model.Then we use polynomial fit for different hyperparameters.In addition to that ,we also train it with those values.At the end , we use the best degree for it. We also eliminated several columns and determined how many could be omitted in order to improve the model.
We divided our data into 15 percent test and 15 percent validation.All the remaining data being used for testing. The values p=3 and d=3 generated the best results.With the hyperparameters shifted, the speed of the algorithms did not improve nor changed significantly.
"""